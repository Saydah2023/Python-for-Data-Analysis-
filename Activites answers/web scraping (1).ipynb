{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c1deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page is found\n"
     ]
    }
   ],
   "source": [
    "#1.Write a Python program to test if a given page is found or not on the server.\n",
    "\n",
    "import requests\n",
    "\n",
    "url=\"https://google.com\"\n",
    "response= requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"page is found\")\n",
    "else:\n",
    "    print(\"page is not on the server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0fa954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n"
     ]
    }
   ],
   "source": [
    "#2.Write a Python program to extract and display all the header tags from en.wikipedia.org/wiki/Main_Page\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response= requests.get(url).text\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "headers=soup.find_all(['h1','h2','h3','h4','h5'])\n",
    "print(*headers, sep=\"\\n\")# '*' is for extract elements form list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1040cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chrome': 2705436259, 'Safari': 2000426074, 'Edge': 476087970, 'Firefox': 126148188, 'Safari (in-app)': 101339847, 'Samsung Internet': 86188652, 'Android Webview': 52138968, 'Opera': 16640788, 'Internet Explorer': 17387732, 'Amazon Silk': 7814772, '(not set)': 5516266, 'Mozilla Compatible Agent': 2649406, 'YaBrowser': 2022681, 'UC Browser': 811581, 'Whale Browser': 569194, 'Coc Coc': 261670, 'Opera Mini': 326280, 'Aloha Browser': 115064, 'Android Runtime': 95580, 'DuckDuckGo Browser': 43505, 'PaleMoon': 48361, 'Phoenix Browser': 44105, 'Android Browser': 52420, 'Iron': 34723, 'SeaMonkey': 43805, 'Mozilla': 50852, 'Vivaldi': 26932, 'Meta Quest Browser': 26634, 'Waterfox': 13297, 'Puffin': 6294, 'Lynx': 210981, '(other)': 356205, 'Seznam': 6027, 'Maxthon': 1527, 'BlackBerry': 1203, 'Konqueror': 903, 'NetFront': 919}\n"
     ]
    }
   ],
   "source": [
    "#3.Write a Python program to get 90 days of visits broken down by browser for all sites on data.gov. https://analytics.usa.gov/data/live/browsers.json\n",
    "\n",
    "import json\n",
    "import requests\n",
    "url=\"https://analytics.usa.gov/data/live/browsers.json\"\n",
    "res=json.loads(requests.get(url).content)[\"totals\"][\"browser\"]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b0f39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/static/images/icons/wikipedia.png\n",
      "/static/images/mobile/copyright/wikipedia-wordmark-en.svg\n",
      "/static/images/mobile/copyright/wikipedia-tagline-en.svg\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/a/af/NlaJeffrey1942-43.jpg/220px-NlaJeffrey1942-43.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/c/c5/008315JeffreyTurnbull1941.jpg/260px-008315JeffreyTurnbull1941.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/e/ea/021807CameronJeffrey1941.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/9/92/AC0072JeffreyTruscottKittyhawks1942.jpg/280px-AC0072JeffreyTruscottKittyhawks1942.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/2/26/VIC1689Jeffrey1945.jpg/280px-VIC1689Jeffrey1945.jpg\n",
      "//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png\n",
      "https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\n",
      "/static/images/footer/wikimedia-button.png\n",
      "/static/images/footer/poweredby_mediawiki_88x31.png\n"
     ]
    }
   ],
   "source": [
    "#4.\tWrite a Python program to extract and display all the image links from en.wikipedia.org/wiki/Peter_Jeffrey_(RAAF_officer)\n",
    "\n",
    "url=\"https://en.wikipedia.org/wiki/Peter_Jeffrey_(RAAF_officer)\"\n",
    "response= requests.get(url).text\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "images=soup.find_all(\"img\")\n",
    "for image in images:\n",
    "    src=image.get(\"src\")# used to get internal attributes of image tag\n",
    "    print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5faac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21355\n"
     ]
    }
   ],
   "source": [
    "#5.\tWrite a Python program to get the number of people visiting a U.S. government website right now. Source: https://analytics.usa.gov/data/live/realtime.json\n",
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "url=\"https://analytics.usa.gov/data/live/realtime.json\"\n",
    "res=json.loads(requests.get(url).content)[\"data\"][0][\"active_visitors\"]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e53111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018 RAM 1500 SPORT V6 ECO-DIESEL *FREE ACCIDENT* CAMERA BLUETOOTH LEATHER HEATED SEATS CRUISE ALLOYS', '$25,495*']\n",
      "['2016 RAM 1500 Sport Crew Cab 4x4 | Blowout Special | HEMI V8 | Backup Camera | Sunroof | Remote Start', '$29,997*']\n",
      "['2012 RAM 1500', '$13,000']\n",
      "['2021 Ford F 350 Platinum FX4 8 Foot Box  Big Roof Leather Nav', '$76,988*']\n",
      "['2021 GMC Sierra 3500 DURAMAX DUALLY 4X4!', '$72,900*']\n",
      "['2017 GMC Sierra 1500 Denali Crew Cab Long Box 4WD', '$42,900*']\n",
      "['2015 Toyota Tacoma 4x4 Access Cab 4A', '$26,497*']\n",
      "[\"2022 RAM 1500 Sport 4x4 Crew Cab 5'7 Box\", '$42,499*']\n",
      "['2017 RAM 1500 4 X 4  Quad Cab 140.5  Laramie + ECODIÉSEL + CUIR', '$25,995*']\n",
      "['2019 Ford F 150 Limited', '$43,095*']\n",
      "['2021 RAM 1500 Big Horn', '$44,237*']\n",
      "['2023 RAM 1500 LIMITED LONGHORN', '$94,464*']\n",
      "['2016 Ford F 150 XLT', '$22,899*']\n",
      "['2024 RAM 2500 BIG HORN', '$72,820*']\n",
      "['2023 RAM 1500 EXPRESS', '$48,460*']\n",
      "['2021 Toyota Tundra SR5', '$57,990*']\n",
      "['2023 RAM 1500 TRADESMAN', '$46,284*']\n",
      "['2021 Chevrolet Silverado 1500 LT Trail Boss', '$43,804*']\n",
      "['2023 RAM 1500 TRADESMAN', '$46,284*']\n",
      "['2023 RAM 1500 TRADESMAN', '$46,284*']\n"
     ]
    }
   ],
   "source": [
    "#6.\tWrite a Python program to get the number of followers of a given twitter account. Choose any twitter account of your favorite celebrity\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def parse_category(cat_link):\n",
    "    base_url = 'https://www.kijijiautos.ca'\n",
    "    cat_url = f'{base_url}/{cat_link}/'\n",
    "    category_page = requests.get(cat_url).text\n",
    "    soup = BeautifulSoup(category_page, 'html.parser')\n",
    "    items = soup.find_all('article', attrs={'data-testid': 'SearchResultListItem'})\n",
    "\n",
    "    return items\n",
    "\n",
    "def save_to_file(content, file_name):\n",
    "    with open(file_name, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "def parse_file(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "    # print(type(file_content))\n",
    "    soup = BeautifulSoup(file_content, 'html.parser')\n",
    "    items = soup.find_all('article', attrs={'data-testid': 'SearchResultListItem'})\n",
    "    return items\n",
    "\n",
    "def get_title(item):\n",
    "    item_title = item.find('h2')\n",
    "\n",
    "    return item_title.text\n",
    "\n",
    "def get_price(item):\n",
    "    item_price = item.find('span', attrs={'data-testid': 'searchResultItemPrice'})\n",
    "\n",
    "    return item_price.text\n",
    "\n",
    "cat_slug = 'cars/pickup-truck'\n",
    "cat_items = parse_category(cat_slug)\n",
    "\n",
    "\n",
    "save_to_file(str(cat_items), 'cars.txt')\n",
    "\n",
    "file_content = parse_file('cars.txt')\n",
    "# print(type(file_content))\n",
    "# print(file_content)\n",
    "\n",
    "for article in file_content:\n",
    "    item_data = []\n",
    "    item_title = get_title(article)\n",
    "    item_price = get_price(article)\n",
    "\n",
    "    item_data.append(item_title)\n",
    "    item_data.append(item_price)\n",
    "\n",
    "\n",
    "    print(item_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70b08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers:{'date': 'Thu, 29 Feb 2024 17:51:17 GMT', 'server': 'mw-web.codfw.main-5c74c7bc9-vfzb8', 'x-content-type-options': 'nosniff', 'content-language': 'en', 'accept-ch': '', 'vary': 'Accept-Encoding,Cookie', 'last-modified': 'Thu, 29 Feb 2024 17:51:16 GMT', 'content-type': 'text/html; charset=UTF-8', 'content-encoding': 'gzip', 'age': '2168', 'x-cache': 'cp4041 miss, cp4041 hit/5409', 'x-cache-status': 'hit-front', 'server-timing': 'cache;desc=\"hit-front\", host;desc=\"cp4041\"', 'strict-transport-security': 'max-age=106384710; includeSubDomains; preload', 'report-to': '{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }', 'nel': '{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}', 'set-cookie': 'WMF-Last-Access=29-Feb-2024;Path=/;HttpOnly;secure;Expires=Mon, 01 Apr 2024 12:00:00 GMT, WMF-Last-Access-Global=29-Feb-2024;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Mon, 01 Apr 2024 12:00:00 GMT, WMF-DP=4bc;Path=/;HttpOnly;secure;Expires=Fri, 01 Mar 2024 00:00:00 GMT, GeoIP=CA:AB:Calgary:51.12:-113.94:v4; Path=/; secure; Domain=.wikipedia.org, NetworkProbeLimit=0.001;Path=/;Secure;Max-Age=3600', 'x-client-ip': '2604:3d09:1786:da00:d9a9:f3ae:e5ca:b1d9', 'cache-control': 'private, s-maxage=0, max-age=0, must-revalidate', 'accept-ranges': 'bytes', 'content-length': '22481'}\n",
      "raw:<urllib3.response.HTTPResponse object at 0x000001D1F8E2E380>\n",
      "url:https://en.wikipedia.org/wiki/Main_Page\n",
      "encoding:UTF-8\n",
      "history:[]\n",
      "reason:OK\n",
      "cookies:<RequestsCookieJar[<Cookie WMF-Last-Access=29-Feb-2024 for en.wikipedia.org/>, <Cookie WMF-DP=4bc for en.wikipedia.org/>, <Cookie NetworkProbeLimit=0.001 for en.wikipedia.org/>, <Cookie WMF-Last-Access-Global=29-Feb-2024 for .wikipedia.org/>, <Cookie GeoIP=CA:AB:Calgary:51.12:-113.94:v4 for .wikipedia.org/>]>\n",
      "elapsed:0:00:01.111229\n",
      "request:<PreparedRequest [GET]>\n",
      "connection:<requests.adapters.HTTPAdapter object at 0x000001D1FA314F90>\n"
     ]
    }
   ],
   "source": [
    "#8.\tWrite a Python program to display the contains of different attributes like different attributes like status_code, headers, url, history, encoding, reason, cookies, elapsed, request and content of a specified resource. \n",
    "\n",
    "\n",
    "url=\"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "res= requests.get(url)\n",
    "for method in res.__dict__:\n",
    "    try:\n",
    "        if '_' not in method:\n",
    "            print(f'{method}:{getattr(res,method)}')\n",
    "    except TypeError:\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aca80c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Domain\n"
     ]
    }
   ],
   "source": [
    "#9.Write a Python program to extract h1 tag from example.com.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://www.example.com'\n",
    "res=requests.get(url).text\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup.find(\"h1\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e16e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292,219\n"
     ]
    }
   ],
   "source": [
    "#10.Write a Python program to get the number of datasets currently listed on data.gov.\n",
    "url='https://www.data.gov'\n",
    "res=requests.get(url).text\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup.find(\"h4\").find(\"span\").text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
